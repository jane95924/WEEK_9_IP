{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WEEK_9_IP_NAIVE BAYES",
      "provenance": [],
      "authorship_tag": "ABX9TyNp10zn3qaafdIOjtQt9OJM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jane95924/WEEK_9_IP/blob/master/WEEK_9_IP_NAIVE_BAYES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uor3qlt2XuBe",
        "colab_type": "text"
      },
      "source": [
        "SPAM MAIL CLASSIFICATION."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iir5mjgbN0QI",
        "colab_type": "text"
      },
      "source": [
        "DEFINING THE QUESTION\n",
        "\n",
        "a. Specifying the question\n",
        "\n",
        " Our task is to predict whether mail sent is spam or not\n",
        "\n",
        "b.Defining the metrics of success\n",
        "  We are classifying, we will use an accuracy score to assess our model. an  accuracy score of 80% is good, this means that we will have correctly classified if it is spam or not.\n",
        "\n",
        "c. Understanding the context\n",
        "    \n",
        "  The number of spam mails received nowadays has gone high and finding a way to filter them is everyone's dream come true. Spam mails can be troublesome to many as they waste time, occupy alot of space and reduces the communication bandwidth. We will use the Naive Bayes classifier to classify the mail if they are spam or ham.\n",
        "d. Recording  experimental design\n",
        "\n",
        "  i. Loading data and initial data exploration\n",
        "       \n",
        "  load the uploaded data, preview the head and tail.\n",
        "\n",
        "  Check the datatypes,shape and the columns in the dataset.\n",
        "\n",
        "  ii. Data cleaning\n",
        "\n",
        "  check for missing values, duplicates,outliers and deal with them.\n",
        "  \n",
        "  iii. EDA\n",
        "\n",
        "  Perform univariate,bivariate and multivariate analysis.  \n",
        "  \n",
        "  iv. Modelling\n",
        "\n",
        "  Use Naive Bayes Classifier\n",
        "  \n",
        "   v. conclusion and challenging the solution\n",
        "\n",
        "e. Data Relevance\n",
        " Our dataset contains 58 columns and 4601 rows, this is a large dataset. A description of the dataset is not provided, we have no null values in our dataset. the columns are appropriate in answering our research question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3OWGzRwCMCB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6b9c0743-1022-4bae-8cf5-71baeea4b315"
      },
      "source": [
        "#importing the libraries we will need\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHz4y2WqCx0m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "a98adce0-dd1a-4ae4-b869-7b5323a9f165"
      },
      "source": [
        "#loading the uploaded dataset\n",
        "data = pd.read_csv('/content/spambase_csv.csv')\n",
        "#previewig the head\n",
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>word_freq_receive</th>\n",
              "      <th>word_freq_will</th>\n",
              "      <th>word_freq_people</th>\n",
              "      <th>word_freq_report</th>\n",
              "      <th>word_freq_addresses</th>\n",
              "      <th>word_freq_free</th>\n",
              "      <th>word_freq_business</th>\n",
              "      <th>word_freq_email</th>\n",
              "      <th>word_freq_you</th>\n",
              "      <th>word_freq_credit</th>\n",
              "      <th>word_freq_your</th>\n",
              "      <th>word_freq_font</th>\n",
              "      <th>word_freq_000</th>\n",
              "      <th>word_freq_money</th>\n",
              "      <th>word_freq_hp</th>\n",
              "      <th>word_freq_hpl</th>\n",
              "      <th>word_freq_george</th>\n",
              "      <th>word_freq_650</th>\n",
              "      <th>word_freq_lab</th>\n",
              "      <th>word_freq_labs</th>\n",
              "      <th>word_freq_telnet</th>\n",
              "      <th>word_freq_857</th>\n",
              "      <th>word_freq_data</th>\n",
              "      <th>word_freq_415</th>\n",
              "      <th>word_freq_85</th>\n",
              "      <th>word_freq_technology</th>\n",
              "      <th>word_freq_1999</th>\n",
              "      <th>word_freq_parts</th>\n",
              "      <th>word_freq_pm</th>\n",
              "      <th>word_freq_direct</th>\n",
              "      <th>word_freq_cs</th>\n",
              "      <th>word_freq_meeting</th>\n",
              "      <th>word_freq_original</th>\n",
              "      <th>word_freq_project</th>\n",
              "      <th>word_freq_re</th>\n",
              "      <th>word_freq_edu</th>\n",
              "      <th>word_freq_table</th>\n",
              "      <th>word_freq_conference</th>\n",
              "      <th>char_freq_%3B</th>\n",
              "      <th>char_freq_%28</th>\n",
              "      <th>char_freq_%5B</th>\n",
              "      <th>char_freq_%21</th>\n",
              "      <th>char_freq_%24</th>\n",
              "      <th>char_freq_%23</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.29</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.778</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.756</td>\n",
              "      <td>61</td>\n",
              "      <td>278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.28</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word_freq_make  word_freq_address  ...  capital_run_length_total  class\n",
              "0            0.00               0.64  ...                       278      1\n",
              "1            0.21               0.28  ...                      1028      1\n",
              "2            0.06               0.00  ...                      2259      1\n",
              "3            0.00               0.00  ...                       191      1\n",
              "4            0.00               0.00  ...                       191      1\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdhULpQgDNqf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "68f81483-f5e4-495e-8e87-58c16465da1b"
      },
      "source": [
        "#previewing the tail\n",
        "data.tail()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>word_freq_receive</th>\n",
              "      <th>word_freq_will</th>\n",
              "      <th>word_freq_people</th>\n",
              "      <th>word_freq_report</th>\n",
              "      <th>word_freq_addresses</th>\n",
              "      <th>word_freq_free</th>\n",
              "      <th>word_freq_business</th>\n",
              "      <th>word_freq_email</th>\n",
              "      <th>word_freq_you</th>\n",
              "      <th>word_freq_credit</th>\n",
              "      <th>word_freq_your</th>\n",
              "      <th>word_freq_font</th>\n",
              "      <th>word_freq_000</th>\n",
              "      <th>word_freq_money</th>\n",
              "      <th>word_freq_hp</th>\n",
              "      <th>word_freq_hpl</th>\n",
              "      <th>word_freq_george</th>\n",
              "      <th>word_freq_650</th>\n",
              "      <th>word_freq_lab</th>\n",
              "      <th>word_freq_labs</th>\n",
              "      <th>word_freq_telnet</th>\n",
              "      <th>word_freq_857</th>\n",
              "      <th>word_freq_data</th>\n",
              "      <th>word_freq_415</th>\n",
              "      <th>word_freq_85</th>\n",
              "      <th>word_freq_technology</th>\n",
              "      <th>word_freq_1999</th>\n",
              "      <th>word_freq_parts</th>\n",
              "      <th>word_freq_pm</th>\n",
              "      <th>word_freq_direct</th>\n",
              "      <th>word_freq_cs</th>\n",
              "      <th>word_freq_meeting</th>\n",
              "      <th>word_freq_original</th>\n",
              "      <th>word_freq_project</th>\n",
              "      <th>word_freq_re</th>\n",
              "      <th>word_freq_edu</th>\n",
              "      <th>word_freq_table</th>\n",
              "      <th>word_freq_conference</th>\n",
              "      <th>char_freq_%3B</th>\n",
              "      <th>char_freq_%28</th>\n",
              "      <th>char_freq_%5B</th>\n",
              "      <th>char_freq_%21</th>\n",
              "      <th>char_freq_%24</th>\n",
              "      <th>char_freq_%23</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4596</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.142</td>\n",
              "      <td>3</td>\n",
              "      <td>88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4597</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.555</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4598</th>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.718</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.404</td>\n",
              "      <td>6</td>\n",
              "      <td>118</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4599</th>\n",
              "      <td>0.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.147</td>\n",
              "      <td>5</td>\n",
              "      <td>78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4600</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.97</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.250</td>\n",
              "      <td>5</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      word_freq_make  word_freq_address  ...  capital_run_length_total  class\n",
              "4596            0.31                0.0  ...                        88      0\n",
              "4597            0.00                0.0  ...                        14      0\n",
              "4598            0.30                0.0  ...                       118      0\n",
              "4599            0.96                0.0  ...                        78      0\n",
              "4600            0.00                0.0  ...                        40      0\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuEbH7uRHGnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1800e007-1ee1-4275-a214-a7f96b9b250f"
      },
      "source": [
        "#checking the shape of our dataset\n",
        "data.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4601, 58)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0x6tYVbHOPc",
        "colab_type": "text"
      },
      "source": [
        "our dataset has 58 columns and 4601 rows. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp-YMj47hS7E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "88e4e204-e766-4f1f-8d85-a35fda58c995"
      },
      "source": [
        "#check the data types of the columns\n",
        "data.dtypes"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "word_freq_make                float64\n",
              "word_freq_address             float64\n",
              "word_freq_all                 float64\n",
              "word_freq_3d                  float64\n",
              "word_freq_our                 float64\n",
              "word_freq_over                float64\n",
              "word_freq_remove              float64\n",
              "word_freq_internet            float64\n",
              "word_freq_order               float64\n",
              "word_freq_mail                float64\n",
              "word_freq_receive             float64\n",
              "word_freq_will                float64\n",
              "word_freq_people              float64\n",
              "word_freq_report              float64\n",
              "word_freq_addresses           float64\n",
              "word_freq_free                float64\n",
              "word_freq_business            float64\n",
              "word_freq_email               float64\n",
              "word_freq_you                 float64\n",
              "word_freq_credit              float64\n",
              "word_freq_your                float64\n",
              "word_freq_font                float64\n",
              "word_freq_000                 float64\n",
              "word_freq_money               float64\n",
              "word_freq_hp                  float64\n",
              "word_freq_hpl                 float64\n",
              "word_freq_george              float64\n",
              "word_freq_650                 float64\n",
              "word_freq_lab                 float64\n",
              "word_freq_labs                float64\n",
              "word_freq_telnet              float64\n",
              "word_freq_857                 float64\n",
              "word_freq_data                float64\n",
              "word_freq_415                 float64\n",
              "word_freq_85                  float64\n",
              "word_freq_technology          float64\n",
              "word_freq_1999                float64\n",
              "word_freq_parts               float64\n",
              "word_freq_pm                  float64\n",
              "word_freq_direct              float64\n",
              "word_freq_cs                  float64\n",
              "word_freq_meeting             float64\n",
              "word_freq_original            float64\n",
              "word_freq_project             float64\n",
              "word_freq_re                  float64\n",
              "word_freq_edu                 float64\n",
              "word_freq_table               float64\n",
              "word_freq_conference          float64\n",
              "char_freq_%3B                 float64\n",
              "char_freq_%28                 float64\n",
              "char_freq_%5B                 float64\n",
              "char_freq_%21                 float64\n",
              "char_freq_%24                 float64\n",
              "char_freq_%23                 float64\n",
              "capital_run_length_average    float64\n",
              "capital_run_length_longest      int64\n",
              "capital_run_length_total        int64\n",
              "class                           int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGrfkzEGhg3_",
        "colab_type": "text"
      },
      "source": [
        "our dataset has the right data types. Our target variable Class is int and we are changing to category since we are classifying "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdZ8n9ITYMkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#changing the target variable data type\n",
        "data['class']=data['class'].astype('category')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD56LXCOHL8p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "5c814af8-287b-4a33-a25a-9f48313d56ac"
      },
      "source": [
        "#look at the columns in our dataset\n",
        "data.columns"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d',\n",
              "       'word_freq_our', 'word_freq_over', 'word_freq_remove',\n",
              "       'word_freq_internet', 'word_freq_order', 'word_freq_mail',\n",
              "       'word_freq_receive', 'word_freq_will', 'word_freq_people',\n",
              "       'word_freq_report', 'word_freq_addresses', 'word_freq_free',\n",
              "       'word_freq_business', 'word_freq_email', 'word_freq_you',\n",
              "       'word_freq_credit', 'word_freq_your', 'word_freq_font', 'word_freq_000',\n",
              "       'word_freq_money', 'word_freq_hp', 'word_freq_hpl', 'word_freq_george',\n",
              "       'word_freq_650', 'word_freq_lab', 'word_freq_labs', 'word_freq_telnet',\n",
              "       'word_freq_857', 'word_freq_data', 'word_freq_415', 'word_freq_85',\n",
              "       'word_freq_technology', 'word_freq_1999', 'word_freq_parts',\n",
              "       'word_freq_pm', 'word_freq_direct', 'word_freq_cs', 'word_freq_meeting',\n",
              "       'word_freq_original', 'word_freq_project', 'word_freq_re',\n",
              "       'word_freq_edu', 'word_freq_table', 'word_freq_conference',\n",
              "       'char_freq_%3B', 'char_freq_%28', 'char_freq_%5B', 'char_freq_%21',\n",
              "       'char_freq_%24', 'char_freq_%23', 'capital_run_length_average',\n",
              "       'capital_run_length_longest', 'capital_run_length_total', 'class'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywek5ZGxa_ik",
        "colab_type": "text"
      },
      "source": [
        "External Data Source Validation. \n",
        "\n",
        "I could not find a similar data source to the dataset provided, but from my research on spam mails, frequent words that pop on spam mails have been included in the dataset. Our dataset is appropriate for our question. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc3SEmVggC21",
        "colab_type": "text"
      },
      "source": [
        "##DATA CLEANING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wDJSXjWHddN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b77c0ef-c41d-48ee-ed3d-ca7fda333468"
      },
      "source": [
        "#checking for null values\n",
        "data.isnull().sum()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "word_freq_make                0\n",
              "word_freq_address             0\n",
              "word_freq_all                 0\n",
              "word_freq_3d                  0\n",
              "word_freq_our                 0\n",
              "word_freq_over                0\n",
              "word_freq_remove              0\n",
              "word_freq_internet            0\n",
              "word_freq_order               0\n",
              "word_freq_mail                0\n",
              "word_freq_receive             0\n",
              "word_freq_will                0\n",
              "word_freq_people              0\n",
              "word_freq_report              0\n",
              "word_freq_addresses           0\n",
              "word_freq_free                0\n",
              "word_freq_business            0\n",
              "word_freq_email               0\n",
              "word_freq_you                 0\n",
              "word_freq_credit              0\n",
              "word_freq_your                0\n",
              "word_freq_font                0\n",
              "word_freq_000                 0\n",
              "word_freq_money               0\n",
              "word_freq_hp                  0\n",
              "word_freq_hpl                 0\n",
              "word_freq_george              0\n",
              "word_freq_650                 0\n",
              "word_freq_lab                 0\n",
              "word_freq_labs                0\n",
              "word_freq_telnet              0\n",
              "word_freq_857                 0\n",
              "word_freq_data                0\n",
              "word_freq_85                  0\n",
              "word_freq_technology          0\n",
              "word_freq_1999                0\n",
              "word_freq_parts               0\n",
              "word_freq_pm                  0\n",
              "word_freq_direct              0\n",
              "word_freq_cs                  0\n",
              "word_freq_meeting             0\n",
              "word_freq_original            0\n",
              "word_freq_project             0\n",
              "word_freq_re                  0\n",
              "word_freq_edu                 0\n",
              "word_freq_table               0\n",
              "word_freq_conference          0\n",
              "char_freq_%3B                 0\n",
              "char_freq_%28                 0\n",
              "char_freq_%5B                 0\n",
              "char_freq_%21                 0\n",
              "char_freq_%24                 0\n",
              "char_freq_%23                 0\n",
              "capital_run_length_average    0\n",
              "capital_run_length_longest    0\n",
              "capital_run_length_total      0\n",
              "class                         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y9KnOdzgadu",
        "colab_type": "text"
      },
      "source": [
        "we have no null values in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTVnJHJCgRvq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c3da0f6-7768-42bc-d2dc-66e3957bebae"
      },
      "source": [
        "#check for duplicates\n",
        "data.duplicated().sum()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "391"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAlaGXXkYjx7",
        "colab_type": "text"
      },
      "source": [
        "we have 391 duplicates in our dataset and we are going to drop them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-uv1mv9gktC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we will drop the duplicates\n",
        "data.drop_duplicates(inplace = True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81K8pw4zhB3f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "100c8cd6-979c-4aca-87f5-5ec4cbdce914"
      },
      "source": [
        "#check if the duplicates were dropped\n",
        "data.duplicated().sum()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ9KF-q7hLml",
        "colab_type": "text"
      },
      "source": [
        "we have no duplicates in our dataset now"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeZuiyap-Fso",
        "colab_type": "text"
      },
      "source": [
        "##EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KseRfVohJgY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "22f44d02-40e5-4b6d-e227-b07e94569569"
      },
      "source": [
        "#We take a look at the distribution of the dataset per column\n",
        "sns.countplot(x='class', data=data)\n",
        "plt.title('count of spam mails')\n",
        "plt.ylabel('count')\n",
        "plt.xlabel('spam email')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'spam email')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Uu82cA-c4y",
        "colab_type": "text"
      },
      "source": [
        "most of the mails received were not spam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sP5Zms3ZLyF",
        "colab_type": "text"
      },
      "source": [
        "Our dataset has so many columns, analysis each column can be difficult,\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpTZRttxaO9p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "9ae6f44f-b247-417a-f8c1-2d69c8cb0a26"
      },
      "source": [
        "sns.scatterplot(x='word_freq_credit',y='class', data=data)\n",
        "plt.title('Word credit versus Class ')\n",
        "plt.ylabel('class')\n",
        "plt.xlabel('word_credit')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'word_credit')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlOOKic5anKB",
        "colab_type": "text"
      },
      "source": [
        "from above, we can see that most mails sent were spam that had word credit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR65E4U3-wvC",
        "colab_type": "text"
      },
      "source": [
        "##NAIVE BAYES CLASSIFIER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK8sWxzpFnfF",
        "colab_type": "text"
      },
      "source": [
        "Naive Bayes assumes each variable is independent\n",
        "\n",
        "It has a high accuracy and high speed with huge datasets\n",
        "\n",
        "Works well with uncorrelated variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUOnVL97-LnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check the correlation of the variables \n",
        "corr = data.corr().abs()\n",
        "corr\n",
        "\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n",
        "\n",
        "# Find index of feature columns with correlation greater than 0.95\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
        "\n",
        "# Drop the highly correlated features \n",
        "data.drop(data[to_drop], axis=1, inplace = True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH6z4l0aa0Qp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1922f705-30b7-45a5-d14e-6dc7bed723d8"
      },
      "source": [
        "#check if any column was dropped.\n",
        "to_drop"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['word_freq_415']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb1KFCRyAu-J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ca91b76-89a9-493f-e824-ebd10662c016"
      },
      "source": [
        "#check if any columns were dropped\n",
        "data.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4210, 57)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_tglFLjEkaL",
        "colab_type": "text"
      },
      "source": [
        "one column has been dropped from our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSbWe_GSBqoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we separate the data to dependent and independent variables.\n",
        "y = data['class']\n",
        "X = data.drop(['class'],axis = 1)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhGmyaYpHrmt",
        "colab_type": "text"
      },
      "source": [
        "Our variables are continuous , we are going to use a Gaussian Nave Bayes classifier\n",
        "Gaussian makes the assumption that our data is normally distributed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFqS0yWuidn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-acJumY9hpD8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43f539a5-10fe-4fa7-f6a9-b34f5adc9ed3"
      },
      "source": [
        "#Creating a baseline model.\n",
        "# Splitting our data into a training set and a test set\n",
        "# \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# Training our model\n",
        "# \n",
        "clf = GaussianNB()  \n",
        "model = clf.fit(X_train, y_train)\n",
        "\n",
        "# Predicting our test predictors\n",
        "predicted = model.predict(X_test)\n",
        "print(np.mean(predicted == y_test))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8230403800475059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-L6gQtUh77R",
        "colab_type": "text"
      },
      "source": [
        "Our baseline model has an accuracy of 82%. it is good but we need to make our model better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAklguZkGfJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting our data into a training set and a test set\n",
        "# \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzVfkRXjGx5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scaling our Data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvUz0T8iG8rH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "27104aa3-1b53-4a69-b8a8-3cf4a46e9089"
      },
      "source": [
        "# Reducing the dimensions in our dataset\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "lda = LDA(n_components=10)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(56, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj2cfbxCHcas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training our model\n",
        "# \n",
        "clf = GaussianNB()  \n",
        "model = clf.fit(X_train, y_train)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhselTfmIIP9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72d6a339-b2fc-44ea-945b-685c23c091d4"
      },
      "source": [
        "# Predicting our test predictors\n",
        "predicted = model.predict(X_test)\n",
        "print(np.mean(predicted == y_test))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9014251781472684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM93ozpfJV-q",
        "colab_type": "text"
      },
      "source": [
        "We got an accuracy of 90.14%. our model is doing good so far.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMnyRA1ec3km",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c2fccddf-0b42-49ff-8584-a0bf92719381"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(y_test, predicted))\n",
        "print(classification_report(y_test, predicted))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[467  28]\n",
            " [ 55 292]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.94      0.92       495\n",
            "           1       0.91      0.84      0.88       347\n",
            "\n",
            "    accuracy                           0.90       842\n",
            "   macro avg       0.90      0.89      0.90       842\n",
            "weighted avg       0.90      0.90      0.90       842\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-65DS7-drWS",
        "colab_type": "text"
      },
      "source": [
        "From our confusion matrix, 28 mails were wrongly labelled as spam and 55 were wrongly labelled as non spam. This is a small number if we compare to the total number. our model is doing good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDunbxcgRE1U",
        "colab_type": "text"
      },
      "source": [
        "We now split the data to 70%train and 30% test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6BX0YywIPeb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "8b6da0b6-8bba-4bf5-e6ed-ff65c991bc08"
      },
      "source": [
        "# Splitting our data into a training set and a test set\n",
        "# \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "#scaling our data\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "#reducing our variables using LDA\n",
        "lda = LDA(n_components=10)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)\n",
        "\n",
        "# Training our model\n",
        "# \n",
        "clf = GaussianNB()  \n",
        "model = clf.fit(X_train, y_train)\n",
        "\n",
        "# Predicting our test predictors\n",
        "predicted = model.predict(X_test)\n",
        "print(np.mean(predicted == y_test))\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9081551860649247\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(56, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbZTXgUxR_dQ",
        "colab_type": "text"
      },
      "source": [
        "Our accuracy score is better than when we did split our data 80-20%. our accuracy score rose to 90.82%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS2HY05geIlQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6244dcfe-0ece-45f0-be2e-c5faf99fd5aa"
      },
      "source": [
        "print(confusion_matrix(y_test, predicted))\n",
        "print(classification_report(y_test, predicted))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[693  44]\n",
            " [ 72 454]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.94      0.92       737\n",
            "           1       0.91      0.86      0.89       526\n",
            "\n",
            "    accuracy                           0.91      1263\n",
            "   macro avg       0.91      0.90      0.90      1263\n",
            "weighted avg       0.91      0.91      0.91      1263\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6RlyfFLeNDV",
        "colab_type": "text"
      },
      "source": [
        "the numbers of wrongly labeled elements are more than in our first model. 44 mails were wronly labelled as spam and 72 were wrongly labelled as non spam. our accuracy increased to 91%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tatGgjgWSL8Q",
        "colab_type": "text"
      },
      "source": [
        "We do split our data to 60% train and 40% test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bquTxpiR-Gf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "85c0e51c-c593-4411-b202-a80ec9c4af42"
      },
      "source": [
        "# Splitting our data into a training set and a test set\n",
        "# \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
        "\n",
        "#scaling our data\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "#reducing our variables using LDA\n",
        "lda = LDA(n_components=10)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)\n",
        "\n",
        "# Training our model\n",
        "# \n",
        "clf = GaussianNB()  \n",
        "model = clf.fit(X_train, y_train)\n",
        "\n",
        "# Predicting our test predictors\n",
        "predicted = model.predict(X_test)\n",
        "print(np.mean(predicted == y_test))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.899643705463183\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(56, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ygRa__sSfhb",
        "colab_type": "text"
      },
      "source": [
        "our accuracy dropped by a very little margin. we have an accuracy of 90%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPXD3uDiejkc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8c3ea33e-219c-4139-ead9-5f7221565ae9"
      },
      "source": [
        "print(confusion_matrix(y_test, predicted))\n",
        "print(classification_report(y_test, predicted))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[933  61]\n",
            " [108 582]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.94      0.92       994\n",
            "           1       0.91      0.84      0.87       690\n",
            "\n",
            "    accuracy                           0.90      1684\n",
            "   macro avg       0.90      0.89      0.90      1684\n",
            "weighted avg       0.90      0.90      0.90      1684\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QYSSNqPem2N",
        "colab_type": "text"
      },
      "source": [
        "The numbers are increasing. More elements are being labelled wrongly.61 false postives and 108 false negatives. we got an accuracy of 90%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhtsFySrSlaa",
        "colab_type": "text"
      },
      "source": [
        "To make our model perform better,\n",
        "\n",
        "1. We can normalize the data\n",
        "2. Apply smoothing techniques in case where we have frequent zeroes in our dataset.\n",
        "We already standardized our data, our model perfomance improved.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdqmrPCghNSH",
        "colab_type": "text"
      },
      "source": [
        "Conclusion and challenging the solution\n",
        "\n",
        "Our model generally did well, attaining an accuracy of 90% in each of the model.\n",
        "Comparing our models to the baseline model accuracy, standardizing the variables and reducing them optimized our model. Our model is reliable, we got a high accuracy score, our metrics of success was achieved. our model did well."
      ]
    }
  ]
}